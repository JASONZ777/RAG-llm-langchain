{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBPBfaLSB+JfcGQvvdV/iX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JASONZ777/RAG-llm-langchain-interface/blob/main/rag_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insall packages"
      ],
      "metadata": {
        "id": "3E95NrCDXqwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/rag-chatbot')"
      ],
      "metadata": {
        "id": "2t_qaQCL_gqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee85d632-8579-4558-9679-5b567ed6a550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkN6xhx3V3MJ"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install unstructured[pdf] --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install optimum --quiet\n",
        "!pip install auto-gptq==0.4.2 --extra-index-url --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import shutil\n",
        "import openai\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "CHROMA_PATH = './chroma'\n",
        "DATA_PATH = './data'\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a helpfula and professional AI Assistant. Given the\n",
        "following conversation history: {chat_history}, please answer the follow up question with help of the context.\n",
        "Question: {query}\n",
        "Context: {context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2iS5QrD_XxVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload pdf files & chunk"
      ],
      "metadata": {
        "id": "lb_wyZFWaWCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to write file uploading as a seperate function because it is a seperated interface"
      ],
      "metadata": {
        "id": "N_wAz-q-O8mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload(file_list):\n",
        "  for file in file_list:\n",
        "    base_name = os.path.basename(file)\n",
        "    new_path = os.path.join('./data', base_name)\n",
        "    shutil.copy2(file, new_path)\n",
        "  return 'Successfully uploaded'"
      ],
      "metadata": {
        "id": "GVK9SrTiM8gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf2chunk(DATA_PATH):\n",
        "    loader = DirectoryLoader(DATA_PATH, glob='*.pdf')\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "yHawC_D7aagn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve & construct prompts"
      ],
      "metadata": {
        "id": "ruEmMSZlb6rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk2context(chunks,query):\n",
        "  database = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH)\n",
        "  database.persist()  # store in the hard disk\n",
        "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())\n",
        "  matches = db.similarity_search_with_relevance_scores(query, k=5)\n",
        "  context = \"\\n\\--\\n\".join([doc.page_content for doc, _score in matches])\n",
        "\n",
        "  return db, context"
      ],
      "metadata": {
        "id": "8jkk3Ks-cdc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline model: llama2-7b-chat"
      ],
      "metadata": {
        "id": "bp1dhKbSZfXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(model_id):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype=torch.float16,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\",\n",
        "      quantization_config=quantization_config\n",
        "  )\n",
        "  generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "  generation_config.max_new_tokens = 1024\n",
        "  generation_config.temperature = 0.1\n",
        "  generation_config.top_p = 0.95\n",
        "  generation_config.do_sample = True\n",
        "  generation_config.repetition_penalty = 1.15\n",
        "\n",
        "  text_pipeline = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      generation_config=generation_config,\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "  return llm"
      ],
      "metadata": {
        "id": "Ji4BG9ETYuPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Follow-up Q/A"
      ],
      "metadata": {
        "id": "6_9-4fNofuM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(model,context,query):\n",
        "  prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "  qa_chain = ConversationChain(\n",
        "      llm=model,\n",
        "      memory=memory,\n",
        "      condense_question_prompt=PROMPT_TEMPLATE,\n",
        "  )\n",
        "  response = qa_chain.predict(context=context,query=query)\n",
        "  return response"
      ],
      "metadata": {
        "id": "5Q_n61SsfzpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine all components & UI"
      ],
      "metadata": {
        "id": "b3TydNv_7xBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)  # clear all stuff, make sure the UI supports multiple questions\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    shutil.rmtree(DATA_PATH)  # clear all stuff\n",
        "os.mkdir(DATA_PATH)\n",
        "\n",
        "def Arabica(query, history):\n",
        "  chunks = pdf2chunk(DATA_PATH)\n",
        "  db, context = chunk2context(chunks,query)\n",
        "\n",
        "  model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "  llm = model(model_id)\n",
        "\n",
        "  response_text = chatbot(llm, context, query)\n",
        "  return response_text\n",
        "\n",
        "file_upload = gr.Interface(fn=upload, inputs=gr.File(file_count='multiple', file_types=['.pdf']),outputs=gr.Text())\n",
        "chat = gr.ChatInterface(fn=Arabica,title=\"Arabicabot\")\n",
        "demo = gr.TabbedInterface([file_upload, chat], [\"Additional files\", \"Arabicabot\"])\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "TdRxO8TW8q9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b08a154-460f-4d32-dd87-d2c94fb2cfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://b15fb1790b2062a384.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b15fb1790b2062a384.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split 3 documents into 1070 chunks.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 235, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1627, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1171, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 657, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 463, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"<ipython-input-12-27e78affaddb>\", line 10, in Arabica\n",
            "    db, context = chunk2context(chunks,query)\n",
            "  File \"<ipython-input-6-5754af503b13>\", line 2, in chunk2context\n",
            "    database = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 180, in warn_if_direct_instance\n",
            "    return wrapped(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for OpenAIEmbeddings\n",
            "__root__\n",
            "  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)\n"
          ]
        }
      ]
    }
  ]
}