{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JASONZ777/RAG-llm-langchain-interface/blob/main/rag_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E95NrCDXqwh"
      },
      "source": [
        "# Insall packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t_qaQCL_gqV",
        "outputId": "4d5d631a-539d-4c8c-f912-ac5b43a5de7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/rag-chatbot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkN6xhx3V3MJ"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install unstructured[pdf] --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install optimum --quiet\n",
        "!pip install auto-gptq==0.4.2 --extra-index-url --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iS5QrD_XxVk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import shutil\n",
        "import openai\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "CHROMA_PATH = './chroma'\n",
        "DATA_PATH = './data'\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a helpfula and professional AI Assistant. Given the\n",
        "following conversation history: {chat_history}, please answer the follow up question with help of the context.\n",
        "Question: {query}\n",
        "Context: {context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hla8ayLvG7l"
      },
      "outputs": [],
      "source": [
        "%env OPENAI_API_KEY=\"OPENAI_API_KEY\"\n",
        "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb_wyZFWaWCy"
      },
      "source": [
        "# Upload pdf files & chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_wAz-q-O8mC"
      },
      "source": [
        "we need to write file uploading as a seperate function because it is a seperated interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVK9SrTiM8gb"
      },
      "outputs": [],
      "source": [
        "def upload(file_list):\n",
        "  for file in file_list:\n",
        "    base_name = os.path.basename(file)\n",
        "    new_path = os.path.join('./data', base_name)\n",
        "    shutil.copy2(file, new_path)\n",
        "  return 'Successfully uploaded'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHawC_D7aagn"
      },
      "outputs": [],
      "source": [
        "def pdf2chunk(DATA_PATH):\n",
        "    loader = DirectoryLoader(DATA_PATH, glob='*.pdf')\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruEmMSZlb6rm"
      },
      "source": [
        "# Retrieve & construct prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jkk3Ks-cdc1"
      },
      "outputs": [],
      "source": [
        "def chunk2context(chunks,query):\n",
        "  database = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH)\n",
        "  database.persist()  # store in the hard disk\n",
        "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())\n",
        "  matches = db.similarity_search_with_relevance_scores(query, k=3)\n",
        "  context = \"\\n\\--\\n\".join([doc.page_content for doc, _score in matches])\n",
        "\n",
        "  return db, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp1dhKbSZfXD"
      },
      "source": [
        "# Pipeline model: llama2-7b-chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji4BG9ETYuPE"
      },
      "outputs": [],
      "source": [
        "def model(model_id):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype=torch.float16,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\",\n",
        "      quantization_config=quantization_config\n",
        "  )\n",
        "  generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "  generation_config.max_new_tokens = 1024\n",
        "  generation_config.temperature = 0.1\n",
        "  generation_config.top_p = 0.95\n",
        "  generation_config.do_sample = True\n",
        "  generation_config.repetition_penalty = 1.15\n",
        "\n",
        "  text_pipeline = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      generation_config=generation_config,\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "  return llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_9-4fNofuM0"
      },
      "source": [
        "# Follow-up Q/A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q_n61SsfzpV"
      },
      "outputs": [],
      "source": [
        "def chatbot(model,context,query):\n",
        "  prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "  qa_chain = ConversationChain(\n",
        "      llm=model,\n",
        "      memory=memory,\n",
        "      condense_question_prompt=PROMPT_TEMPLATE,\n",
        "  )\n",
        "  response = qa_chain.predict(context=context,query=query)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3TydNv_7xBO"
      },
      "source": [
        "# Combine all components & UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TdRxO8TW8q9S"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)  # clear all stuff, make sure the UI supports multiple questions\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    shutil.rmtree(DATA_PATH)  # clear all stuff\n",
        "os.mkdir(DATA_PATH)\n",
        "\n",
        "def Arabica(query, history):\n",
        "  chunks = pdf2chunk(DATA_PATH)\n",
        "  db, context = chunk2context(chunks,query)\n",
        "\n",
        "  model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "  llm = model(model_id)\n",
        "\n",
        "  response_text = chatbot(llm, context, query)\n",
        "  return response_text\n",
        "\n",
        "file_upload = gr.Interface(fn=upload, inputs=gr.File(file_count='multiple', file_types=['.pdf']),outputs=gr.Text())\n",
        "chat = gr.ChatInterface(fn=Arabica,title=\"Arabicabot\")\n",
        "demo = gr.TabbedInterface([file_upload, chat], [\"Additional files\", \"Arabicabot\"])\n",
        "demo.launch(debug=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2vWu5/ad2guuxdeVFfIn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}